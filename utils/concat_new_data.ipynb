{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 ufo json파일 -> coco  (x) tag추가 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tag를 달아놔도 cvat 작업을 하면 tag 사라짐.\n",
    "\n",
    "# from typing import Dict\n",
    "# import json\n",
    "# import datetime\n",
    "# import os\n",
    "\n",
    "# now = datetime.datetime.now()\n",
    "# now = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# input_path = '../data/medical/ufo/divided_train.json'\n",
    "# # input_path = '../../data/medical/ufo/train.json'\n",
    "# output_path = '../data/medical/ufo/train_coco.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info = {\n",
    "#     'year': 2024,\n",
    "#     'version': '1.0',\n",
    "#     'description': 'OCR Competition Data',\n",
    "#     'contributor': 'Naver Boostcamp',\n",
    "#     'url': 'https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000273/data/data.tar.gz',\n",
    "#     'date_created': now\n",
    "# }\n",
    "# licenses = {\n",
    "#     'id': '1',\n",
    "#     'name': 'For Naver Boostcamp Competition',\n",
    "#     'url': None\n",
    "# }\n",
    "# categories = [{\n",
    "#     'id': 1,\n",
    "#     'name': 'word'\n",
    "# }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ufo_to_coco(file: Dict, output_path: str) -> None:\n",
    "#     img_id = 1 #COCO는 1부터 시작\n",
    "#     annotation_id = 1 #COCO는 1부터 시작\n",
    "#     images = []\n",
    "#     annotations = []\n",
    "#     for fname, data in file.items():\n",
    "#         image = {\n",
    "#             \"id\": img_id,\n",
    "#             \"width\": data['img_w'],\n",
    "#             \"height\": data['img_h'],\n",
    "#             \"file_name\": fname,\n",
    "#             \"license\": 1,\n",
    "#             \"flickr_url\": None,\n",
    "#             \"coco_url\": None,\n",
    "#             \"date_captured\": now\n",
    "#         }\n",
    "#         images.append(image)\n",
    "#         for anno_id, annotation in data['words'].items():\n",
    "#             if annotation['illegibility'] == True:\n",
    "#                 continue\n",
    "#             min_x = min(item[0] for item in annotation['points'])\n",
    "#             min_y = min(item[1] for item in annotation['points'])\n",
    "#             max_x = max(item[0] for item in annotation['points'])\n",
    "#             max_y = max(item[1] for item in annotation['points'])\n",
    "#             width = max_x - min_x\n",
    "#             height = max_y - min_y\n",
    "#             tags = annotation['tags']\n",
    "\n",
    "#             coco_annotation = {\n",
    "#                 \"id\": annotation_id,\n",
    "#                 \"image_id\": img_id,\n",
    "#                 \"category_id\": 1,\n",
    "#                 \"segmentation\": [[value for sublist in annotation['points'] for value in sublist]],\n",
    "#                 \"area\": width * height,\n",
    "#                 \"bbox\": [min_x, min_y, width, height],\n",
    "#                 \"iscrowd\": 0,\n",
    "#                 'tags' : tags\n",
    "#             }\n",
    "#             annotations.append(coco_annotation)\n",
    "#             annotation_id += 1\n",
    "#         img_id += 1\n",
    "#     coco = {\n",
    "#         'info' : info,\n",
    "#         'images' : images,\n",
    "#         'annotations' : annotations,\n",
    "#         'licenses' : licenses,\n",
    "#         'categories' : categories\n",
    "#     }\n",
    "#     with open(output_path, 'w') as f:\n",
    "#         json.dump(coco, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(input_path, 'r') as f:\n",
    "#     file = json.load(f)\n",
    "# ufo_to_coco(file['images'], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 파일 coco -> ufo (x) 기본 tag 추가 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Dict\n",
    "# import json\n",
    "# import datetime\n",
    "\n",
    "\n",
    "# now = datetime.datetime.now()\n",
    "# now = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# input_path = '../data/medical/ufo/train_coco.json'\n",
    "# output_path = '../data/medical/ufo/train_coco_2_ufo.json'\n",
    "\n",
    "# ufo = {\n",
    "#     'images': {}\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def coco_bbox_to_ufo(bbox):\n",
    "#     min_x, min_y, width, height = bbox\n",
    "#     return [\n",
    "#         [min_x, min_y],\n",
    "#         [min_x + width, min_y],\n",
    "#         [min_x + width, min_y + height],\n",
    "#         [min_x, min_y + height]\n",
    "#     ]\n",
    "\n",
    "# def coco_to_ufo(file: Dict, output_path: str) -> None:\n",
    "#     anno_id = 1\n",
    "#     for annotation in file['annotations']:\n",
    "#         file_info = file['images'][int(annotation['image_id'])-1]\n",
    "#         image_name = file_info['file_name']\n",
    "#         if image_name not in ufo['images']:\n",
    "#             anno_id = 1\n",
    "#             ufo['images'][image_name] = {\n",
    "#                 \"paragraphs\": {},\n",
    "#                 \"words\": {},\n",
    "#                 \"chars\": {},\n",
    "#                 \"img_w\": file_info[\"width\"],\n",
    "#                 \"img_h\": file_info[\"height\"],\n",
    "#                 \"tags\": [\"autoannotated\"],\n",
    "#                 \"relations\": {},\n",
    "#                 \"annotation_log\": {\n",
    "#                     \"worker\": \"\",\n",
    "#                     \"timestamp\": now,\n",
    "#                     \"tool_version\": \"LabelMe or CVAT\",\n",
    "#                     \"source\": None\n",
    "#                     },\n",
    "#                 \"license_tag\": {\n",
    "#                     \"usability\": True,\n",
    "#                     \"public\": False,\n",
    "#                     \"commercial\": True,\n",
    "#                     \"type\": None,\n",
    "#                     \"holder\": \"Upstage\"\n",
    "#                     }\n",
    "#                 }\n",
    "            \n",
    "#             # anno_id = 1\n",
    "#         ufo['images'][image_name]['words'][str(anno_id).zfill(4)] = {\n",
    "#             \"transcription\": \"\",\n",
    "#             \"points\":  coco_bbox_to_ufo(annotation[\"bbox\"]),\n",
    "#             \"orientation\": \"Horizontal\",\n",
    "#             \"language\": None,\n",
    "#             \"tags\": annotation['tags'],  ## 이거 변경\n",
    "#             \"confidence\": None,\n",
    "#             \"illegibility\": False\n",
    "#         }\n",
    "#         anno_id += 1\n",
    "\n",
    "#     with open(output_path, \"w\") as f:\n",
    "#         json.dump(ufo, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(input_path, 'r') as f:\n",
    "#     file = json.load(f)\n",
    "# coco_to_ufo(file, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 ufo json파일 -> coco  (o) tag 제외 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore list에 포함된 정보를 가진 경우 bbox 삭제\n",
    "\n",
    "from typing import Dict\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "input_path = '../data/medical/ufo/divided_train.json'\n",
    "# input_path = '../../data/medical/ufo/train.json'\n",
    "output_path = '../data/medical/ufo/train_coco.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "    'year': 2024,\n",
    "    'version': '1.0',\n",
    "    'description': 'OCR Competition Data',\n",
    "    'contributor': 'Naver Boostcamp',\n",
    "    'url': 'https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000273/data/data.tar.gz',\n",
    "    'date_created': now\n",
    "}\n",
    "licenses = {\n",
    "    'id': '1',\n",
    "    'name': 'For Naver Boostcamp Competition',\n",
    "    'url': None\n",
    "}\n",
    "categories = [{\n",
    "    'id': 1,\n",
    "    'name': 'word'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_skip(annotation_tags, tags_to_skip):\n",
    "    for tag in annotation_tags:\n",
    "        if tag in tags_to_skip:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def ufo_to_coco(file: Dict, output_path: str) -> None:\n",
    "    img_id = 1 #COCO는 1부터 시작\n",
    "    annotation_id = 1 #COCO는 1부터 시작\n",
    "    images = []\n",
    "    annotations = []\n",
    "    for fname, data in file.items():\n",
    "        image = {\n",
    "            \"id\": img_id,\n",
    "            \"width\": data['img_w'],\n",
    "            \"height\": data['img_h'],\n",
    "            \"file_name\": fname,\n",
    "            \"license\": 1,\n",
    "            \"flickr_url\": None,\n",
    "            \"coco_url\": None,\n",
    "            \"date_captured\": now\n",
    "        }\n",
    "        images.append(image)\n",
    "        for anno_id, annotation in data['words'].items():\n",
    "            if annotation['illegibility'] == True:\n",
    "                continue\n",
    "            elif should_skip(annotation['tags'], ['masked', 'excluded-region', 'maintable', 'stamp']) :#ignore_list\n",
    "                continue\n",
    "            min_x = min(item[0] for item in annotation['points'])\n",
    "            min_y = min(item[1] for item in annotation['points'])\n",
    "            max_x = max(item[0] for item in annotation['points'])\n",
    "            max_y = max(item[1] for item in annotation['points'])\n",
    "            width = max_x - min_x\n",
    "            height = max_y - min_y\n",
    "\n",
    "            coco_annotation = {\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": 1,\n",
    "                \"segmentation\": [[value for sublist in annotation['points'] for value in sublist]],\n",
    "                \"area\": width * height,\n",
    "                \"bbox\": [min_x, min_y, width, height],\n",
    "                \"iscrowd\": 0,\n",
    "            }\n",
    "            annotations.append(coco_annotation)\n",
    "            annotation_id += 1\n",
    "        img_id += 1\n",
    "    coco = {\n",
    "        'info' : info,\n",
    "        'images' : images,\n",
    "        'annotations' : annotations,\n",
    "        'licenses' : licenses,\n",
    "        'categories' : categories\n",
    "    }\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(coco, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path, 'r') as f:\n",
    "    file = json.load(f)\n",
    "ufo_to_coco(file['images'], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 작업한 coco json파일 -> ufo  (o) tag 제외 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "now = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "input_path = '../data/medical/ufo/instances_default.json'\n",
    "output_path = '../data/medical/ufo/train_coco_2_ufo.json'\n",
    "\n",
    "ufo = {\n",
    "    'images': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_bbox_to_ufo(bbox):\n",
    "    min_x, min_y, width, height = bbox\n",
    "    return [\n",
    "        [min_x, min_y],\n",
    "        [min_x + width, min_y],\n",
    "        [min_x + width, min_y + height],\n",
    "        [min_x, min_y + height]\n",
    "    ]\n",
    "\n",
    "def coco_to_ufo(file: Dict, output_path: str) -> None:\n",
    "    anno_id = 1\n",
    "    for annotation in file['annotations']:\n",
    "        file_info = file['images'][int(annotation['image_id'])-1]\n",
    "        image_name = file_info['file_name']\n",
    "        if image_name not in ufo['images']:\n",
    "            anno_id = 1\n",
    "            ufo['images'][image_name] = {\n",
    "                \"paragraphs\": {},\n",
    "                \"words\": {},\n",
    "                \"chars\": {},\n",
    "                \"img_w\": file_info[\"width\"],\n",
    "                \"img_h\": file_info[\"height\"],\n",
    "                \"tags\": [\"autoannotated\"],\n",
    "                \"relations\": {},\n",
    "                \"annotation_log\": {\n",
    "                    \"worker\": \"\",\n",
    "                    \"timestamp\": now,\n",
    "                    \"tool_version\": \"LabelMe or CVAT\",\n",
    "                    \"source\": None\n",
    "                    },\n",
    "                \"license_tag\": {\n",
    "                    \"usability\": True,\n",
    "                    \"public\": False,\n",
    "                    \"commercial\": True,\n",
    "                    \"type\": None,\n",
    "                    \"holder\": \"Upstage\"\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # anno_id = 1\n",
    "        ufo['images'][image_name]['words'][str(anno_id).zfill(4)] = {\n",
    "            \"transcription\": \"\",\n",
    "            \"points\":  coco_bbox_to_ufo(annotation[\"bbox\"]),\n",
    "            \"orientation\": \"Horizontal\",\n",
    "            \"language\": None,\n",
    "            \"tags\": 'Auto',  ## 이거 변경\n",
    "            \"confidence\": None,\n",
    "            \"illegibility\": False\n",
    "        }\n",
    "        anno_id += 1\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(ufo, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path, 'r') as f:\n",
    "    file = json.load(f)\n",
    "coco_to_ufo(file, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 옮겨둔 이미지와 일치하는 json파일 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/OCR/img'  # 폴더 경로\n",
    "files_ext = os.listdir(folder_path)  # 폴더 내의 파일 목록\n",
    "# 확장자를 제외한 파일명만 추출\n",
    "img_files = [os.path.splitext(file)[0] for file in files_ext]\n",
    "len(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# json 파일이 있는 폴더 경로\n",
    "json_source_folder = \"../data/medical/OCR/01.라벨링데이터(Json)\"\n",
    "\n",
    "# 이동시킬 폴더\n",
    "destination_json_folder = \"../data/medical/OCR/json\"\n",
    "\n",
    "# json 파일을 복사하며 이미지 파일과 동일한 이름인 경우에만 복사\n",
    "for root, dirs, files in os.walk(json_source_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            source_file_path = os.path.join(root, file)\n",
    "            destination_json_name = os.path.splitext(file)[0]  # 확장자 제외한 파일 이름만 가져오기\n",
    "            destination_json_path = os.path.join(destination_json_folder, file)\n",
    "            if destination_json_name in img_files:\n",
    "                shutil.copy(source_file_path, destination_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_folder_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/OCR/json'  # 폴더 경로\n",
    "json_files_ext = os.listdir(json_folder_path)  # 폴더 내의 파일 목록\n",
    "len(json_files_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom -> coco json파일 합치기 // 공공행정문서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "    'year': 2024,\n",
    "    'version': '1.0',\n",
    "    'description': 'OCR Competition Data',\n",
    "    'contributor': 'Naver Boostcamp',\n",
    "    'url': 'https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000273/data/data.tar.gz',\n",
    "}\n",
    "licenses = {\n",
    "    'id': '1',\n",
    "    'name': 'For Naver Boostcamp Competition',\n",
    "    'url': None\n",
    "}\n",
    "categories = [{\n",
    "    'id': 1,\n",
    "    'name': 'word'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# JSON 파일이 있는 폴더 경로\n",
    "json_folder = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/OCR/json/'\n",
    "output_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/new_json.json'\n",
    "\n",
    "# 기존 정보\n",
    "info = {\n",
    "    'year': 2024,\n",
    "    'version': '1.0',\n",
    "    'description': 'OCR Competition Data',\n",
    "    'contributor': 'Naver Boostcamp',\n",
    "    'url': 'https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000273/data/data.tar.gz',\n",
    "}\n",
    "licenses = {\n",
    "    'id': '1',\n",
    "    'name': 'For Naver Boostcamp Competition',\n",
    "    'url': None\n",
    "}\n",
    "categories = [{'id': 1, 'name': 'word'}]\n",
    "\n",
    "# COCO 데이터 초기화\n",
    "img_id = 1\n",
    "annotation_id = 1\n",
    "images = []\n",
    "annotations = []\n",
    "\n",
    "for file_name in os.listdir(json_folder):\n",
    "    if file_name.endswith('.json'):\n",
    "        input_path = os.path.join(json_folder, file_name)\n",
    "\n",
    "        with open(input_path, 'r') as f:\n",
    "            file = json.load(f)\n",
    "        image = {\n",
    "            'id': img_id,\n",
    "            'width': file['images'][0]['image.width'],\n",
    "            'height': file['images'][0]['image.height'],\n",
    "            'file_name': file['images'][0][\"image.file.name\"],\n",
    "            \"license\": 1,\n",
    "            \"flickr_url\": None,\n",
    "            \"coco_url\": None,\n",
    "            'data_captured': file['images'][0][\"image.create.time\"]\n",
    "        }\n",
    "        images.append(image)\n",
    "\n",
    "        for ann_info in file['annotations']:\n",
    "            min_x = ann_info[\"annotation.bbox\"][0]\n",
    "            min_y = ann_info[\"annotation.bbox\"][1]\n",
    "            width = ann_info[\"annotation.bbox\"][2]\n",
    "            height = ann_info[\"annotation.bbox\"][3]\n",
    "\n",
    "            segmentation = [\n",
    "                            [min_x, min_y, min_x + width, min_y, min_x + width, min_y + height, min_x, min_y + height]\n",
    "                            ]\n",
    "\n",
    "            coco_annotation = {\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": 1,\n",
    "                \"segmentation\": segmentation,\n",
    "                \"area\": width * height,\n",
    "                \"bbox\": [min_x, min_y, width, height],\n",
    "                \"iscrowd\": 0,\n",
    "                'tags' : ['Auto']\n",
    "            }\n",
    "            annotations.append(coco_annotation)\n",
    "            annotation_id += 1\n",
    "\n",
    "        img_id += 1\n",
    "\n",
    "# 모든 데이터를 COCO 포맷으로 합치기\n",
    "coco = {\n",
    "    'info': info,\n",
    "    'images': images,\n",
    "    'annotations': annotations,\n",
    "    'licenses': licenses,  # 리스트로 변환\n",
    "    'categories': categories\n",
    "}\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(coco, f, indent=4)\n",
    "\n",
    "# KeyError 0 나면, new_json.json가 이미 만들어져 있는 거임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coco -> ufo // 공공행정문서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# 커스텀 coco포맷 json파일 -> ufo포맷\n",
    "# input_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/new_json.json'\n",
    "# output_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/_new_json.json'\n",
    "\n",
    "# cvat작업 coco포맷 json파일 -> ufo포맷\n",
    "input_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/instances_default.json'\n",
    "output_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/_instances_default.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo = {\n",
    "    'images': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_bbox_to_ufo(bbox):\n",
    "    min_x, min_y, width, height = bbox\n",
    "    return [\n",
    "        [min_x, min_y],\n",
    "        [min_x + width, min_y],\n",
    "        [min_x + width, min_y + height],\n",
    "        [min_x, min_y + height]\n",
    "    ]\n",
    "\n",
    "def coco_to_ufo(file: Dict, output_path: str) -> None:\n",
    "    anno_id = 1\n",
    "    for annotation in file['annotations']:\n",
    "        file_info = file['images'][int(annotation['image_id'])-1]\n",
    "        image_name = file_info['file_name']\n",
    "        if image_name not in ufo['images']:\n",
    "            anno_id = 1\n",
    "            ufo['images'][image_name] = {\n",
    "                \"paragraphs\": {},\n",
    "                \"words\": {},\n",
    "                \"chars\": {},\n",
    "                \"img_w\": file_info[\"width\"],\n",
    "                \"img_h\": file_info[\"height\"],\n",
    "                \"tags\": [\"autoannotated\"],\n",
    "                \"relations\": {},\n",
    "                \"annotation_log\": {\n",
    "                    \"worker\": \"\",\n",
    "                    \"timestamp\": now,\n",
    "                    \"tool_version\": \"LabelMe or CVAT\",\n",
    "                    \"source\": None\n",
    "                    },\n",
    "                \"license_tag\": {\n",
    "                    \"usability\": True,\n",
    "                    \"public\": False,\n",
    "                    \"commercial\": True,\n",
    "                    \"type\": None,\n",
    "                    \"holder\": \"Upstage\"\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # anno_id = 1\n",
    "        ufo['images'][image_name]['words'][str(anno_id).zfill(4)] = {\n",
    "            \"transcription\": \"\",\n",
    "            \"points\":  coco_bbox_to_ufo(annotation[\"bbox\"]),\n",
    "            \"orientation\": \"Horizontal\",\n",
    "            \"language\": None,\n",
    "            \"tags\": ['Auto'],\n",
    "            \"confidence\": None,\n",
    "            \"illegibility\": False\n",
    "        }\n",
    "        anno_id += 1\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(ufo, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     file \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcoco_to_ufo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m, in \u001b[0;36mcoco_to_ufo\u001b[0;34m(file, output_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m         ufo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m][image_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparagraphs\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 }\n\u001b[1;32m     38\u001b[0m             }\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m# anno_id = 1\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     ufo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m][image_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mstr\u001b[39m(anno_id)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m4\u001b[39m)] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m:  coco_bbox_to_ufo(annotation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHorizontal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mannotation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124millegibility\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     }\n\u001b[1;32m     50\u001b[0m     anno_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tags'"
     ]
    }
   ],
   "source": [
    "with open(input_path, 'r') as f:\n",
    "    file = json.load(f)\n",
    "coco_to_ufo(file, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom -> coco json파일 합치기 // 금융물류문서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "    'year': 2024,\n",
    "    'version': '1.0',\n",
    "    'description': 'OCR Competition Data',\n",
    "    'contributor': 'Naver Boostcamp',\n",
    "    'url': 'https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000273/data/data.tar.gz',\n",
    "}\n",
    "licenses = {\n",
    "    'id': '1',\n",
    "    'name': 'For Naver Boostcamp Competition',\n",
    "    'url': None\n",
    "}\n",
    "categories = [{\n",
    "    'id': 1,\n",
    "    'name': 'word'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# JSON 파일이 있는 폴더 경로\n",
    "json_folder = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/sample_ocr/json/'\n",
    "output_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/add_json.json'\n",
    "\n",
    "# 기존 정보\n",
    "info = {\n",
    "    'year': 2024,\n",
    "    'version': '1.0',\n",
    "    'description': 'OCR Competition Data',\n",
    "    'contributor': 'Naver Boostcamp',\n",
    "    'url': 'https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000273/data/data.tar.gz',\n",
    "}\n",
    "licenses = {\n",
    "    'id': '1',\n",
    "    'name': 'For Naver Boostcamp Competition',\n",
    "    'url': None\n",
    "}\n",
    "categories = [{'id': 1, 'name': 'word'}]\n",
    "\n",
    "# COCO 데이터 초기화\n",
    "img_id = 1\n",
    "annotation_id = 1\n",
    "images = []\n",
    "annotations = []\n",
    "\n",
    "for file_name in os.listdir(json_folder):\n",
    "    if file_name.endswith('.json'):\n",
    "        input_path = os.path.join(json_folder, file_name)\n",
    "\n",
    "        with open(input_path, 'r') as f:\n",
    "            file = json.load(f)\n",
    "        image = {\n",
    "            'id': img_id,\n",
    "            'width': file['Images']['width'],\n",
    "            'height': file['Images']['height'],\n",
    "            'file_name': file['Images'][\"identifier\"],\n",
    "            \"license\": 1,\n",
    "            \"flickr_url\": None,\n",
    "            \"coco_url\": None,\n",
    "            'data_captured': file['Images'][\"data_captured\"]\n",
    "        }\n",
    "        images.append(image)\n",
    "\n",
    "        for ann_info in file['bbox']:\n",
    "            min_x = min(ann_info[\"x\"])\n",
    "            max_x = max(ann_info[\"x\"])\n",
    "            min_y = min(ann_info[\"y\"])\n",
    "            max_y = max(ann_info[\"y\"])\n",
    "\n",
    "            # width와 height 계산\n",
    "            width = max_x - min_x\n",
    "            height = max_y - min_y\n",
    "\n",
    "            segmentation = [\n",
    "                        [min_x, min_y, max_x, min_y, max_x, max_y, min_x, max_y]\n",
    "                            ]\n",
    "\n",
    "            coco_annotation = {\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": 1,\n",
    "                \"segmentation\": segmentation,\n",
    "                \"area\": width * height,\n",
    "                \"bbox\": [min_x, min_y, width, height],\n",
    "                \"iscrowd\": 0,\n",
    "                'tags' : ['Auto']\n",
    "            }\n",
    "            annotations.append(coco_annotation)\n",
    "            annotation_id += 1\n",
    "\n",
    "        img_id += 1\n",
    "\n",
    "# 모든 데이터를 COCO 포맷으로 합치기\n",
    "coco = {\n",
    "    'info': info,\n",
    "    'images': images,\n",
    "    'annotations': annotations,\n",
    "    'licenses': licenses,  # 리스트로 변환\n",
    "    'categories': categories\n",
    "}\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(coco, f, indent=4)\n",
    "\n",
    "# KeyError 0 나면, new_json.json가 이미 만들어져 있는 거임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coco -> ufo // 금융물류문서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# 커스텀 coco포맷 json파일 -> ufo포맷\n",
    "# input_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/add_json.json'\n",
    "# output_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/_add_json.json'\n",
    "\n",
    "# cvat작업 coco포맷 json파일 -> ufo포맷\n",
    "input_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/instances_default.json'\n",
    "output_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/_instances_default.json'\n",
    "\n",
    "ufo = {\n",
    "    'images': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_bbox_to_ufo(bbox):\n",
    "    min_x, min_y, width, height = bbox\n",
    "    return [\n",
    "        [min_x, min_y],\n",
    "        [min_x + width, min_y],\n",
    "        [min_x + width, min_y + height],\n",
    "        [min_x, min_y + height]\n",
    "    ]\n",
    "\n",
    "def coco_to_ufo(file: Dict, output_path: str) -> None:\n",
    "    anno_id = 1\n",
    "    for annotation in file['annotations']:\n",
    "        file_info = file['images'][int(annotation['image_id'])-1]\n",
    "        image_name = file_info['file_name']\n",
    "        if image_name not in ufo['images']:\n",
    "            anno_id = 1\n",
    "            ufo['images'][image_name] = {\n",
    "                \"paragraphs\": {},\n",
    "                \"words\": {},\n",
    "                \"chars\": {},\n",
    "                \"img_w\": file_info[\"width\"],\n",
    "                \"img_h\": file_info[\"height\"],\n",
    "                \"tags\": [\"autoannotated\"],\n",
    "                \"relations\": {},\n",
    "                \"annotation_log\": {\n",
    "                    \"worker\": \"\",\n",
    "                    \"timestamp\": now,\n",
    "                    \"tool_version\": \"LabelMe or CVAT\",\n",
    "                    \"source\": None\n",
    "                    },\n",
    "                \"license_tag\": {\n",
    "                    \"usability\": True,\n",
    "                    \"public\": False,\n",
    "                    \"commercial\": True,\n",
    "                    \"type\": None,\n",
    "                    \"holder\": \"Upstage\"\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # anno_id = 1\n",
    "        ufo['images'][image_name]['words'][str(anno_id).zfill(4)] = {\n",
    "            \"transcription\": \"\",\n",
    "            \"points\":  coco_bbox_to_ufo(annotation[\"bbox\"]),\n",
    "            \"orientation\": \"Horizontal\",\n",
    "            \"language\": None,\n",
    "            \"tags\": ['Auto'],\n",
    "            \"confidence\": None,\n",
    "            \"illegibility\": False\n",
    "        }\n",
    "        anno_id += 1\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(ufo, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path, 'r') as f:\n",
    "    file = json.load(f)\n",
    "coco_to_ufo(file, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 json 파일과 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 json\n",
    "original_json_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/train.json'\n",
    "# custom_json파일\n",
    "made_json_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/_CORD_json.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(original_json_path, 'r') as f:\n",
    "    file_1 = json.load(f)\n",
    "with open(made_json_path, 'r') as f:\n",
    "    file_2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path ='/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/_newest_.json'\n",
    "\n",
    "combined_images = {}\n",
    "combined_images = {**file_1['images'], **file_2['images']}\n",
    "\n",
    "combined_json = {\n",
    "    'images': combined_images\n",
    "}\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(combined_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_1['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_2['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_json['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface_CORD 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.21.3)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.62.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2023.11.17)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.7.1)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Using cached numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tzdata, pyarrow-hotfix, numpy, multidict, frozenlist, dill, async-timeout, yarl, pyarrow, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.3\n",
      "    Uninstalling numpy-1.21.3:\n",
      "      Successfully uninstalled numpy-1.21.3\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.16.1 dill-0.3.7 frozenlist-1.4.1 huggingface-hub-0.20.3 multidict-6.0.4 multiprocess-0.70.15 numpy-1.26.3 pandas-2.2.0 pyarrow-15.0.0 pyarrow-hotfix-0.6 tzdata-2023.4 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.7.1)\n",
      "Collecting widgetsnbextension~=4.0.9 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.9 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.9/214.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.1 jupyterlab-widgets-3.0.9 widgetsnbextension-4.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../data/medical/hugging/train-00000-of-00004-b4aaeceff1d90ecb.parquet')\n",
    "\n",
    "image_dir = '../data/medical/hugging/cord_images'\n",
    "json_dir = '../data/medical/hugging/cord_json'\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(json_dir, exist_ok=True)\n",
    "\n",
    "#이미지 저장\n",
    "for index, row in df.iterrows():\n",
    "    image_data = row['image']['bytes']\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    image.save(f'../data/medical/hugging/cord_images/image_{index+1}.jpg')\n",
    "\n",
    "#json 저장\n",
    "for index, row in df.iterrows():\n",
    "    ground_truth_str = row['ground_truth']  \n",
    "    ground_truth_dict = json.loads(ground_truth_str)\n",
    "    with open(f'../data/medical/hugging/cord_json/image_{index+1}.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(ground_truth_dict, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface_CORD -> COCO json 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "    'year': 2024,\n",
    "    'version': '1.0',\n",
    "    'description': 'OCR Competition Data',\n",
    "    'contributor': 'Naver Boostcamp',\n",
    "    'url': 'https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000273/data/data.tar.gz',\n",
    "}\n",
    "licenses = {\n",
    "    'id': '1',\n",
    "    'name': 'For Naver Boostcamp Competition',\n",
    "    'url': None\n",
    "}\n",
    "categories = [{\n",
    "    'id': 1,\n",
    "    'name': 'word'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# JSON 파일이 있는 폴더 경로\n",
    "json_folder = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/hugging/cord_json/'\n",
    "output_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/CORD_json.json'\n",
    "\n",
    "# 기존 정보\n",
    "info = {\n",
    "    'year': 2024,\n",
    "    'version': '1.0',\n",
    "    'description': 'OCR Competition Data',\n",
    "    'contributor': 'Naver Boostcamp',\n",
    "    'url': 'https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000273/data/data.tar.gz',\n",
    "}\n",
    "licenses = {\n",
    "    'id': '1',\n",
    "    'name': 'For Naver Boostcamp Competition',\n",
    "    'url': None\n",
    "}\n",
    "categories = [{'id': 1, 'name': 'word'}]\n",
    "\n",
    "# COCO 데이터 초기화\n",
    "img_id = 1\n",
    "annotation_id = 1\n",
    "images = []\n",
    "annotations = []\n",
    "\n",
    "file_names = os.listdir(json_folder)\n",
    "sorted_file_names = sorted(file_names, key=lambda x: int(x.split('_')[1].split('.')[0])) # 파일 정렬\n",
    "\n",
    "for file_name in sorted_file_names:\n",
    "    if file_name.endswith('.json') and img_id:\n",
    "        input_path = os.path.join(json_folder, file_name)\n",
    "\n",
    "        with open(input_path, 'r') as f:\n",
    "            file = json.load(f)\n",
    "        image = {\n",
    "            'id': img_id,\n",
    "            'width': file['meta']['image_size']['width'],\n",
    "            'height': file['meta']['image_size']['height'],\n",
    "            'file_name': f'image_{img_id}.jpg',\n",
    "            \"license\": 1,\n",
    "            \"flickr_url\": None,\n",
    "            \"coco_url\": None,\n",
    "            'data_captured': None\n",
    "        }\n",
    "        images.append(image)\n",
    "\n",
    "        for ann_info in file['valid_line']:\n",
    "            for word_info in ann_info['words']:\n",
    "                quad_info = word_info['quad']\n",
    "                x1 = quad_info['x1']\n",
    "                y1 = quad_info['y1']\n",
    "                x2 = quad_info['x2']\n",
    "                y3 = quad_info['y3']\n",
    "\n",
    "                # COCO 형식으로 bbox 좌표 계산\n",
    "                min_x = x1\n",
    "                min_y = y1\n",
    "                width = x2 - x1\n",
    "                height = y3 - y1\n",
    "\n",
    "                segmentation = [\n",
    "                                [min_x, min_y, min_x + width, min_y, min_x + width, min_y + height, min_x, min_y + height]\n",
    "                                ]\n",
    "\n",
    "                coco_annotation = {\n",
    "                    \"id\": annotation_id,\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": 1,\n",
    "                    \"segmentation\": segmentation,\n",
    "                    \"area\": width * height,\n",
    "                    \"bbox\": [min_x, min_y, width, height],\n",
    "                    \"iscrowd\": 0,\n",
    "                    'tags' : ['Auto']\n",
    "                }\n",
    "                annotations.append(coco_annotation)\n",
    "                annotation_id += 1\n",
    "\n",
    "        img_id += 1\n",
    "\n",
    "# 모든 데이터를 COCO 포맷으로 합치기\n",
    "coco = {\n",
    "    'info': info,\n",
    "    'images': images,\n",
    "    'annotations': annotations,\n",
    "    'licenses': licenses,  # 리스트로 변환\n",
    "    'categories': categories\n",
    "}\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(coco, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface_CORD coco -> ufo json 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# 커스텀 coco포맷 json파일 -> ufo포맷\n",
    "# input_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/add_json.json'\n",
    "# output_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/_add_json.json'\n",
    "\n",
    "# cvat작업 coco포맷 json파일 -> ufo포맷\n",
    "input_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/CORD_json.json'\n",
    "output_path = '/data/ephemeral/home/level2-cv-datacentric-cv-10/data/medical/ufo/_CORD_json.json'\n",
    "\n",
    "ufo = {\n",
    "    'images': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_bbox_to_ufo(bbox):\n",
    "    min_x, min_y, width, height = bbox\n",
    "    return [\n",
    "        [min_x, min_y],\n",
    "        [min_x + width, min_y],\n",
    "        [min_x + width, min_y + height],\n",
    "        [min_x, min_y + height]\n",
    "    ]\n",
    "\n",
    "def coco_to_ufo(file: Dict, output_path: str) -> None:\n",
    "    anno_id = 1\n",
    "    for annotation in file['annotations']:\n",
    "        file_info = file['images'][int(annotation['image_id'])-1]\n",
    "        image_name = file_info['file_name']\n",
    "        if image_name not in ufo['images']:\n",
    "            anno_id = 1\n",
    "            ufo['images'][image_name] = {\n",
    "                \"paragraphs\": {},\n",
    "                \"words\": {},\n",
    "                \"chars\": {},\n",
    "                \"img_w\": file_info[\"width\"],\n",
    "                \"img_h\": file_info[\"height\"],\n",
    "                \"tags\": [\"autoannotated\"],\n",
    "                \"relations\": {},\n",
    "                \"annotation_log\": {\n",
    "                    \"worker\": \"\",\n",
    "                    \"timestamp\": now,\n",
    "                    \"tool_version\": \"LabelMe or CVAT\",\n",
    "                    \"source\": None\n",
    "                    },\n",
    "                \"license_tag\": {\n",
    "                    \"usability\": True,\n",
    "                    \"public\": False,\n",
    "                    \"commercial\": True,\n",
    "                    \"type\": None,\n",
    "                    \"holder\": \"Upstage\"\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # anno_id = 1\n",
    "        ufo['images'][image_name]['words'][str(anno_id).zfill(4)] = {\n",
    "            \"transcription\": \"\",\n",
    "            \"points\":  coco_bbox_to_ufo(annotation[\"bbox\"]),\n",
    "            \"orientation\": \"Horizontal\",\n",
    "            \"language\": None,\n",
    "            \"tags\": ['Auto'],\n",
    "            \"confidence\": None,\n",
    "            \"illegibility\": False\n",
    "        }\n",
    "        anno_id += 1\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(ufo, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path, 'r') as f:\n",
    "    file = json.load(f)\n",
    "coco_to_ufo(file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
